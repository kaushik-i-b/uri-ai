# Ollama API configuration
# Replace with the actual URL of your Ollama service
# Examples:
# - For local development: http://localhost:11434
# - For Docker Compose: http://ollama:11434
# - For remote server: http://your-server-ip:11434
OLLAMA_API_URL=http://localhost:11434

# Ollama model to use
# Default is llama2-uncensored, but you can specify any model available in your Ollama instance
OLLAMA_MODEL=llama2-uncensored

# API server configuration
PORT=8000

# Milvus configuration
# Replace with the actual host, port, and collection name for your Milvus service
# Examples:
# - For local development: localhost
# - For Docker Compose: milvus
# - For remote server: your-server-ip
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_COLLECTION=chat_memories